# Linear Regression
Date: 24-12-2021
tags: #ml, #ds

---
## Quick Note


## Detail
Basically, a pair of $(x^i,y^i)$ is called a training examples, where $x^i$ is input and $y^i$ is output.
Dataset includes a list of *m* training samples where *i* = 1, 2,... m. 

![[Pasted image 20211224162046.png|500]]
![[Pasted image 20211224162252.png|500]]

$h_\theta(x)$ is basically our predictor (what we predicted), another way to refer to it is ==hypothesis== and its formula is: $$h_\theta(x) = \theta_0 + \theta_1x$$ where the parameters $\theta_0$ and $\theta_1$ represents the coefficient to be adjusted for the line to fit.

We want to minimize it through [[Cost-function]].

### Cost-Function
![[Cost-function#Detail]]

### Gradient Descent
The above notes talked about hypothesis function and measuring how well it fits into data. To estimate the parameter in the function, we use [[Gradient Descent]]





## Reference
-	Highlight ==whatâ€™s important!==
